{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glamacles/notebooks/blob/main/gaussian_processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee692b2c-fd8c-433c-ba92-e52e4c76b9f4",
      "metadata": {
        "id": "ee692b2c-fd8c-433c-ba92-e52e4c76b9f4"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- We're familiar with random variables\n",
        "- For example a multivariate Gaussian:\n",
        "$$\\pmb{y} \\sim \\mathcal{N}(\\pmb{\\mu}, \\Sigma)$$\n",
        "$$ P(\\pmb{y}) = (2 \\pi)^{-n/2} |\\Sigma|^{-1/2} \\exp \\left ( -\\frac{1}{2} (\\pmb{y} - \\pmb{\\mu})^T \\Sigma^{-1} (\\pmb{y} - \\pmb{\\mu}) \\right ) $$\n",
        "- Samples are points in $n$-dimensional space\n",
        "- A 2D example:"
      ],
      "metadata": {
        "id": "BjMo641Tgfu-"
      },
      "id": "BjMo641Tgfu-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff8d1e0-c676-43e6-aaa4-a6a182eb27bb",
      "metadata": {
        "id": "8ff8d1e0-c676-43e6-aaa4-a6a182eb27bb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "x0 = np.array([0., 0.])\n",
        "sigma = np.array([\n",
        "    [2., -1],\n",
        "    [-1, 2.]\n",
        "])\n",
        "\n",
        "ys = np.random.multivariate_normal(x0, sigma, 500)\n",
        "plt.scatter(ys[:,0], ys[:,1])\n",
        "plt.title('Samples from 2d Gaussian Distribution')\n",
        "plt.grid(True, linestyle=':')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Processes"
      ],
      "metadata": {
        "id": "1AuGUnCDgqhM"
      },
      "id": "1AuGUnCDgqhM"
    },
    {
      "cell_type": "markdown",
      "id": "64b393bb-b4d1-476a-9d43-cb01d7a478a4",
      "metadata": {
        "id": "64b393bb-b4d1-476a-9d43-cb01d7a478a4"
      },
      "source": [
        "\n",
        "- A Gaussian process can be thought of as a generalization of a Gaussian distribution\n",
        "- Instead of a distribution over points, it can be thought of as a distribution over functions\n",
        "- That is, samples are functions instead of points!\n",
        "- A Gaussian process is fully defined by a mean function $m(x)$ and a kernel $k(x,x')$\n",
        "- Kernel represents relationship between \"nearby\" points\n",
        "- For example, consider the Radial Basis Function (RBF) Kernel\n",
        " $$k(x, x') = \\sigma^2 \\exp \\left ( -\\frac{|| x - x' ||^2 }{ 2 l^2 }\\right )$$\n",
        "- Below we'll draw samples from a GP with $m(x)=0$ and an RBF kernel\n",
        "- Nearby points in space are more highly correlated than those further apart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27dd76f-4656-42c4-b1b7-bccdf9197361",
      "metadata": {
        "id": "e27dd76f-4656-42c4-b1b7-bccdf9197361"
      },
      "outputs": [],
      "source": [
        "# X coords\n",
        "xs = np.linspace(0.,1.,100)\n",
        "\n",
        "# Kernel Matrix\n",
        "d = distance_matrix(xs[:,np.newaxis], xs[:,np.newaxis], p=2)\n",
        "# Variance\n",
        "sigma = 1.\n",
        "# Length scale\n",
        "l = 0.1\n",
        "# Kernel\n",
        "K = sigma**2 * np.exp(-d**2 / (2*l**2))\n",
        "\n",
        "\n",
        "plt.title('Samples from a Gaussian Process')\n",
        "# Plot realizations\n",
        "ys = np.random.multivariate_normal(np.zeros_like(xs), K, 500)\n",
        "for i in range(5):\n",
        "    plt.plot(xs, ys[i], label=f'Sample {i}')\n",
        "    plt.xlim([0.,1.])\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a103d2-8c00-40d8-a552-f065fea5b20a",
      "metadata": {
        "id": "67a103d2-8c00-40d8-a552-f065fea5b20a"
      },
      "source": [
        "## Meaning of $m(x)$ and $k(x,x')$\n",
        "- Mean function of a GP $m(x)$ defines the mean of the distribution of functions at a given point\n",
        "- Kernel $k(x,x')$ can characterize the variance around the mean and \"smoothness\" of the samples\n",
        "- In the RBF kernel these are controled by the parameters $\\sigma$ and $l$ respectively\n",
        "- Other choices for the kernel can encode other useful assumptions like periodicity\n",
        "\n",
        "## Effect of RBF Kernel Paramters\n",
        "- Below you can play around with the variance and length scale parameters in an RBF kernel to see how it affects the GP samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9e505c-cb99-4b0f-aa22-f442a857b9b5",
      "metadata": {
        "id": "9b9e505c-cb99-4b0f-aa22-f442a857b9b5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "def plot_function(sigma, l):\n",
        "    xs = np.linspace(0.,1.,100)\n",
        "\n",
        "    # Kernel Matrix\n",
        "    d = distance_matrix(xs[:,np.newaxis], xs[:,np.newaxis], p=2)\n",
        "    # Kernel\n",
        "    K = sigma**2 * np.exp(-d**2 / (2*l**2))\n",
        "\n",
        "    ys = np.random.multivariate_normal(np.zeros_like(xs), K, 5)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.title('Samples from GP with RBF Kernel')\n",
        "    for i in range(5):\n",
        "        plt.plot(xs, ys[i])\n",
        "    plt.xlim([0.,1.])\n",
        "    plt.ylim([-2, 2])\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.show()\n",
        "\n",
        "interact(plot_function, sigma=(0.01, 1.), l=(0.01,0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a90771-5909-4607-9598-bf52f8b3ed74",
      "metadata": {
        "id": "04a90771-5909-4607-9598-bf52f8b3ed74"
      },
      "source": [
        "## Distribution of Random Variables in a GP\n",
        "- A collection of random variables $f(\\pmb{x}_1), \\cdots, f(\\pmb{x_n})$ is said to be drawn from a GP with mean $m(x)$ and covariance function $k(x,x')$ if they have the distribution\n",
        "$$\\pmb{f} \\sim \\mathcal{N} \\left ( \\pmb{m}, K \\right ) $$\n",
        "\n",
        "&emsp;&emsp; where\n",
        "$$ \\pmb{f} = [f(\\pmb{x_1}), f(\\pmb{x_2}), \\cdots, f(\\pmb{x_n})]^T$$\n",
        "$$ \\pmb{m} = [m(\\pmb{x_1}), m(\\pmb{x_2}), \\cdots, m(\\pmb{x_n})]^T$$\n",
        "$$\n",
        "K =\n",
        "\\begin{bmatrix}\n",
        "k(\\pmb{x_1}, \\pmb{x_1}) & \\cdots & k(\\pmb{x_1}, \\pmb{x_n}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "k(\\pmb{x_n}, \\pmb{x_1}) & \\cdots & k(\\pmb{x_n}, \\pmb{x_n})\n",
        "\\end{bmatrix} $$\n",
        "- That is, if we take any set of points $\\pmb{x_1}, \\pmb{x_2}, \\cdots, \\pmb{x_n}$, the random variables corresponding to those points $f(\\pmb{x_1}), f(\\pmb{x_2}), \\cdots, f(\\pmb{x_n})$ follow a multivariate Gaussian distribution\n",
        "- For example, if we look at two arbitrary points $x_1$ and $x_2$, $y$ values of our sample functions form a 2d Gaussian distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83827f4f-b6e0-4232-8e86-1b04eb304a4d",
      "metadata": {
        "id": "83827f4f-b6e0-4232-8e86-1b04eb304a4d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "\n",
        "def plot_function(i0, i1):\n",
        "    plt.clf()\n",
        "\n",
        "    plt.figure(figsize=(6,8))\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.title('GP Samples')\n",
        "    for i in range(25):\n",
        "        plt.plot(xs, ys[i])\n",
        "        plt.plot([xs[i0], xs[i1]], [ys[i,i0], ys[i,i1]], 'ko')\n",
        "    plt.xlim([0,1.])\n",
        "\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.title(f'Distribution of [f({xs[i0]:.2f}), f({xs[i1]:.2f})]')\n",
        "    plt.scatter(ys[:,i0], ys[:,i1])\n",
        "    plt.show()\n",
        "\n",
        "interact(plot_function, i0=(0, 99), i1=(0, 99))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Process Regression"
      ],
      "metadata": {
        "id": "TUZnMuczg1RC"
      },
      "id": "TUZnMuczg1RC"
    },
    {
      "cell_type": "markdown",
      "id": "6f07efe5-55d5-4bba-a9fa-a6798b9a2866",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6f07efe5-55d5-4bba-a9fa-a6798b9a2866"
      },
      "source": [
        "\n",
        "\n",
        "## Scenario\n",
        "- Suppose we have some noisy observations of an unknown function $F : \\mathcal{R}^n \\to \\mathcal{R}$, and we assume\n",
        "$$ \\pmb{y} = F(X) + \\pmb{\\epsilon} $$\n",
        "where, $\\epsilon$ is independent and identically distributed (i.i.d.)\n",
        "$$ \\epsilon \\sim \\mathcal{N} \\left ( \\pmb{0}, \\sigma^2 I \\right ) $$\n",
        "and the notation $F(X)$ means\n",
        "\n",
        "$$ F(X) = [F(\\pmb{x_1}), \\cdots, F(\\pmb{x_{N_0}})] $$\n",
        "&emsp; &emsp; with\n",
        "$$ X = [\\pmb{x_1}, \\cdots, \\pmb{x_{N_0}}] $$\n",
        "- The i.i.d. assumption can be relaxed, but this is a common assumption\n",
        "- Now, suppose that we want to predict outputs of $F$ at unknown locations\n",
        "$$ \\pmb{y_*} = F(X^*) $$\n",
        "where\n",
        "$$X_* = [\\pmb{x_1}_*, \\cdots, \\pmb{x_{N_1}}_*]$$\n",
        "\n",
        "\n",
        "\n",
        "## Joint Distribution\n",
        "- For vanilla Gaussian Process regression, we typically assume that the observations are generated by a GP with a zero mean and given covariance kernel $k(x,x')$\n",
        "- Mean functions can also be inferred, however\n",
        "- Under this assumption, the joint distribution of all points (observed $X$, and unobserved $X_*$) is Gaussian:\n",
        "$$ \\begin{bmatrix}\n",
        "\\pmb{y} \\\\\n",
        "\\pmb{y_*}\n",
        "\\end{bmatrix}\n",
        "\\sim\n",
        "\\mathcal{N} \\left ( \\pmb{0},\n",
        "\\begin{bmatrix}\n",
        " K(X,X) + \\sigma^2 I & K(X,X_*)\\\\\n",
        " K(X_*, X) & K(X_*, X_*)\n",
        "\\end{bmatrix}\n",
        "\\right )$$\n",
        "- The notation $K(X, X_*)$ is shorthand for the given covariance matrix\n",
        "$$\n",
        "K(X, X_*) = \\begin{bmatrix}\n",
        " k(\\pmb{x_1}, \\pmb{{x_1}_*}) & \\cdots & k(\\pmb{x_1}, \\pmb{{x_{N_1}}_*}) \\\\\n",
        " \\vdots & \\ddots & \\vdots \\\\\n",
        " k(\\pmb{x_{N_0}}, \\pmb{{x_1}_*}) & \\cdots & k(\\pmb{x_{N_0}}, \\pmb{{x_{N_1}}_*})\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- $K(X,X)$ and $K(X_*,X)$ are defined similarly\n",
        "\n",
        "## Prediction: Computing the Conditional Distribution\n",
        "- From here, can estimate the distribution for the unknown random variables $\\pmb{y_*}$ at the unobserved points in the matrix $X^*$ by conditioning on the observations\n",
        "- Here, we're using the nice properties of conditional distributions for Gaussians\n",
        "- This yields an estimate of mean and covariance at our prediction points\n",
        "$$ \\pmb{y}^* \\sim \\mathcal{N}( \\pmb{m}, P) $$\n",
        "$$ \\pmb{m} = K(X_*, X) \\left ( K(X, X) + \\sigma^2 I \\right )^{-1} \\pmb{y} $$\n",
        "$$ P = K(X_*, X_*) - K(X_*, X) \\left [ K(X, X) + \\sigma^2 I \\right ]^{-1} K(X, X_*)   $$\n",
        "- If there is no noise, then the $GP$ regressor will match the observations exactly, but this can be numerically unstable\n",
        "\n",
        "## Learning Hyperparamters\n",
        "- Kernel hyperparameters such as the length scale and variance in the RBF can be set explicitly\n",
        "- This is useful for some applications (e.g. when a GP is used as a prior)\n",
        "- In some cases we would rather infer hyperparameters\n",
        "- This is usally done via maximum likelihood estimation (MLE)\n",
        "> MLE: Maximize the likelihood that the observations were drawn from a GP with the given hyperparamters\n",
        "- In the setting of GP regression, assuming Gaussian noise, we already know the distribution of the data points given some paramters $\\theta$\n",
        "$$ P(\\pmb{y}|\\theta) \\sim \\mathcal{N}(\\pmb{0}, K_{\\theta}(X,X) + \\sigma^2 I) $$\n",
        "- The idea is to maximize this probability by varying $\\theta$\n",
        "- In practice, this is typically done by minimizing log likelihood w.r.t. hyperpramters $\\theta$\n",
        "$$ \\log p \\left ( \\pmb{y} | X, \\theta \\right ) = -\\frac{1}{2} \\pmb{y}^T (K_{\\theta}(X,X) + \\sigma^2 I)^{-1} \\pmb{y} - \\frac{1}{2} | K_{\\theta}(X,X) + \\sigma^2 I | - \\frac{n}{2} \\log 2 \\pi$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Implementation"
      ],
      "metadata": {
        "id": "jiBUe4MOjTJg"
      },
      "id": "jiBUe4MOjTJg"
    },
    {
      "cell_type": "markdown",
      "id": "9faf8373-768e-4953-b02b-3c4dd9b89847",
      "metadata": {
        "id": "9faf8373-768e-4953-b02b-3c4dd9b89847"
      },
      "source": [
        "\n",
        "- [Scikit-learn](https://scikit-learn.org) has an easy to use implementation\n",
        "- Below we show a basic example\n",
        "- Another good and very flexible implementation of GP regression is [GPytorch](https://gpytorch.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f134102-19c4-4458-acc2-5c4d8708451f",
      "metadata": {
        "id": "0f134102-19c4-4458-acc2-5c4d8708451f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "X = np.linspace(0., 10., 1000).reshape(-1, 1)\n",
        "\n",
        "# Function to gregress\n",
        "y = np.squeeze(X * np.sin(X))\n",
        "\n",
        "# Randomly choose some x-coordinates where we have observations\n",
        "rng = np.random.RandomState(1)\n",
        "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
        "X_train, y_train = X[training_indices], y[training_indices]\n",
        "\n",
        "# Get noisy observations\n",
        "noise_std = 0.75\n",
        "y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)\n",
        "\n",
        "# Do the Gaussian Process regression using an RBF kernel\n",
        "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
        "gaussian_process = GaussianProcessRegressor(\n",
        "    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n",
        ")\n",
        "gaussian_process.fit(X_train, y_train_noisy)\n",
        "\n",
        "# Predict function and variance at unknown locations\n",
        "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
        "\n",
        "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
        "plt.errorbar(\n",
        "    X_train,\n",
        "    y_train_noisy,\n",
        "    noise_std,\n",
        "    linestyle=\"None\",\n",
        "    color=\"tab:blue\",\n",
        "    marker=\".\",\n",
        "    markersize=10,\n",
        "    label=\"Observations\",\n",
        ")\n",
        "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
        "plt.fill_between(\n",
        "    X.ravel(),\n",
        "    mean_prediction - 1.96 * std_prediction,\n",
        "    mean_prediction + 1.96 * std_prediction,\n",
        "    color=\"tab:orange\",\n",
        "    alpha=0.5,\n",
        "    label=r\"95% confidence interval\",\n",
        ")\n",
        "plt.legend()\n",
        "plt.xlim([0.,10.])\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$f(x)$\")\n",
        "plt.title(\"Gaussian process regression on a noisy dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bea91bd-2763-42a2-919c-6029553f057b",
      "metadata": {
        "id": "2bea91bd-2763-42a2-919c-6029553f057b"
      },
      "source": [
        "## Kernels\n",
        "- There are many choices of the covariance kernel for different applications\n",
        "- One useful kernel is the [Matern kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html)\n",
        "$$ k(x, x') =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n",
        "\\frac{\\sqrt{2\\nu}}{l} d(x , x')\n",
        "\\Bigg)^\\nu K_\\nu\\Bigg(\n",
        "\\frac{\\sqrt{2\\nu}}{l} d(x , x' )\\Bigg)\n",
        "$$\n",
        "- Here $\\Gamma$ is the gamma function, and $K_{\\nu}$ is the modified bessel function of the second time.\n",
        "- For $\\nu \\to \\infty$ this converges to the RBF kernel\n",
        "- With right parameters, functions can be more \"jagged\" than for RBF kernel\n",
        "- Another useful kernel is a [periodic kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html)\n",
        "- Could be useful for regression of seasonal / diurnal signals\n",
        "- GPytorch implements a large number of covariance kernels and allows for custom [kernels](https://docs.gpytorch.ai/en/v1.11/kernels.html)\n",
        "- GPytorch even allows for a learned neural network based covariance kernel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, ExpSineSquared\n",
        "\n",
        "\n",
        "#length_scale = 1.0\n",
        "#nu = 1.5\n",
        "#kernel = Matern(length_scale=length_scale, nu=nu)\n",
        "\n",
        "length_scale = 1.0  # This controls the length scale of the kernel\n",
        "periodicity = 3.0  # This is the period of the kernel\n",
        "kernel = ExpSineSquared(length_scale=length_scale, periodicity=periodicity)\n",
        "\n",
        "gp = GaussianProcessRegressor(kernel=kernel)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "samples = gp.sample_y(X, n_samples=5, random_state=0)\n",
        "\n",
        "# Plotting the samples\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, sample in enumerate(samples.T):\n",
        "    plt.plot(X, sample, lw=2, label=f'Sample {i+1}')\n",
        "\n",
        "plt.title('GP Samples with Periodic Kernel')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.xlim([0,10.])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2gXpdz2_Wnuv"
      },
      "id": "2gXpdz2_Wnuv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Output Dimensions\n",
        "- Note that in the setting outlined here, the points we're predicting values at can be in any dimension as long as the output is scalar\n",
        "- If you want to predict multiple outputs, you can use one GP per output dimension\n",
        "$$\n",
        "\\pmb{y} = [{GP}_1(\\pmb{x}), \\cdots, {GP}_n(\\pmb{x})]\n",
        "$$\n",
        "- Curse of dimensionality applies, so learning very high dimensional input spaces is difficult"
      ],
      "metadata": {
        "id": "zUp4RhXYrfcw"
      },
      "id": "zUp4RhXYrfcw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Inference\n",
        "\n",
        "- Doing inference or training GP hyperparamters scales according to the number of training points. Consider computing the mean and variance of a single test point $\\pmb{x_*}$\n",
        "\n",
        "\n",
        "## Computing the mean\n",
        "- After determining the hyperparamters, the vector  \n",
        "$$ \\pmb{\\alpha} = \\left ( K(X,X) + \\sigma^2 \\right )^{-1} \\pmb{y}$$\n",
        "can be precomputed, for example, using the Cholesky decomposition\n",
        "- Then, computing the mean for a given test point $\\pmb{x_*}$ can be computed by\n",
        "$$ \\pmb{m} = K(X,\\pmb{x^*})^T \\pmb{\\alpha} $$\n",
        "\n",
        "## Computing the variance\n",
        "\n",
        "- Again, for a single test point $\\pmb{x_*}$ computing the variance requires computing\n",
        "$$ v(\\pmb{x_*}) = K(\\pmb{x_*}, X) \\left ( K + \\sigma^2 I \\right ) ^{-1} K(X, \\pmb{x_*}) $$\n",
        "- This can be solved efficiently using the Cholesky decomposition in $\\mathcal{O}(n^2)$\n",
        "\n",
        "\n",
        "## Note on Cholesky Decomposition\n",
        "\n",
        "- If a matrix $A$ is a real and positive-definite (like a covariance matrix), it can be decomposed as\n",
        "$$A = L L^T$$\n",
        "where $L$ is lower triangular and $L^T$ is upper triangular.\n",
        "- Suppose we want to solve $$A\\pmb{x} = L L^T \\pmb{x} = \\pmb{b}$$\n",
        "- First, we can solve $L \\pmb{y} = \\pmb{b} $ by forward substitution.\n",
        "- Then we can solve $L^T \\pmb{x} = \\pmb{y}$ by back substitution.\n",
        "- This has computational complexity $\\mathcal{O}(n^2)$\n",
        "\n"
      ],
      "metadata": {
        "id": "5eFLYXIGocpm"
      },
      "id": "5eFLYXIGocpm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approximate GPs\n",
        "\n"
      ],
      "metadata": {
        "id": "PvTvR2eudQlC"
      },
      "id": "PvTvR2eudQlC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are numerous methods for speeding up GP regression using approximate GPs\n",
        "- One approach is to use inducing point methods\n",
        "- These typically select a subset of the original training points, conditioning only on those points"
      ],
      "metadata": {
        "id": "z27V_isgrWnC"
      },
      "id": "z27V_isgrWnC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications in Glaciology"
      ],
      "metadata": {
        "id": "UTBV_CS0ka4K"
      },
      "id": "UTBV_CS0ka4K"
    },
    {
      "cell_type": "markdown",
      "id": "17872974-32ca-47fc-9ba0-08a334367c73",
      "metadata": {
        "id": "17872974-32ca-47fc-9ba0-08a334367c73"
      },
      "source": [
        "\n",
        "\n",
        "- GP's have many useful applications in Glaciology\n",
        "- Can be used for interpolation of unknown variables\n",
        "- GP's can be used as priors over functions for inverse problems (e.g. [this paper](https://www.sciencedirect.com/science/article/pii/S0021999122001577) on variational inference / other examples [here](https://arxiv.org/pdf/2112.13663))\n",
        "- GP's can be used for emulation (e.g. [Edwards et al. 2021](https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/24263/Edwards_2021_Nature_Projectedlandice_AAM.pdf?sequence=1&isAllowed=y)).\n",
        "- Another cool example is using a GP emulator to help reconstruct ice sheet geometry at [125 ka](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019JF005237)\n",
        "- For most practical use cases GP emulation is paired with some dimensionality reduction like principal components analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1\n",
        "\n",
        "- [Hill et al. 2024](https://essopenarchive.org/users/832963/articles/1226265-computationally-efficient-subglacial-drainage-modeling-using-gaussian-process-emulators-glads-gp-v1-0?commit=d091d843a1a3fc443c8b310c9f4c098c6656e12e) uses GPs to create an emulator for the GlaDS subglacial hydrology model\n",
        "- The main use case is for Bayesian inference of unknown parameters in the hydrology model\n",
        "- Conceptually, a set of parameters is sampled and GlaDS simulations are run for each sample\n",
        "- Given the results, the principal components of the time / spatially dependent simulation outputs are computed\n",
        "- Then, a GP is trained to map parameter inputs to coefficients of the principal component basis functions\n",
        "- This is an efficient way to get spatially / time dependent outputs efficiently\n",
        "- Note, some similarities do a Deep Operator Network\n",
        "- Good basis functions are predetermined, and a model is trained to determine the weights as input parameters change\n",
        "<div>\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/Screenshot 2025-06-20 224428.png?raw=1\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "b6MNO2I9wWLV"
      },
      "id": "b6MNO2I9wWLV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2\n",
        "\n",
        "- [Downs et al. 2022](https://www.cambridge.org/core/journals/journal-of-glaciology/article/inferring-timedependent-calving-dynamics-at-helheim-glacier/D678DCC319C0228A087701B315E4C971) uses a GP emulator to estimate uncertain model parameters for Helheim glacier\n",
        "- Idea is to infer parameters that lead to best agreement between modeled / observed terminus position\n",
        "- An ensemble of model runs is performed with different model parameters\n",
        "- Then a GP is used to estimate mismatch between observations / model for arbitrary parameters based on this ensemble data\n",
        "- GP fills in the gaps between unknown parameters and allows for fast MCMC sampling\n",
        "- FIgure below shows an example of model parameters inferred from MCMC using the GP emulator\n",
        "- This sampling would be intractable without emulation\n",
        "<div>\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/gaussian_processes/emulator.png?raw=1\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "k2srI5ykwSNq"
      },
      "id": "k2srI5ykwSNq"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}