{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glamacles/notebooks/blob/main/gnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Neural Networks and Emulators"
      ],
      "metadata": {
        "id": "frqKA4jhp69O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unstructured Data\n",
        "\n",
        "- CNNs are can be very effective for dealing with problems that have structured data on a  grid\n",
        "- This makes CNNs ideal for raster datasets\n",
        "- But some data is not well suited to a grid based representation\n",
        "- Examples: Finite element meshes, social networks, particle systems, etc."
      ],
      "metadata": {
        "id": "uNQ8GGbcp-HC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/0_KIKnUvzdIkp5zcDJ.jpg?raw=true\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/Left-to-right-uniform-mesh-as-commonly-used-in-finite-differences-scheme-triangle-mesh_W640.jpg?raw=true\" width=\"300\"/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "AgH-frW0tWfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Neural Networks (GNNs)\n",
        "- What is a graph?\n",
        "- A set of vertices $V$ and edges $E$ defining connections among vertices\n",
        "- Notably, grids correspond to a particular kind of graph structure with uniformly spaced vertices and consistent connectivity rules between nearby vertices\n",
        "- Edges can be directed (an edge between vertex $i$ and vertex $j$ does not imply an edge between vertex $j$ and vertex $i$) or undirected"
      ],
      "metadata": {
        "id": "tHpJaWCDv0GP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/local_emulators/graph.png?raw=1\" width=\"400\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "7SZF46rqwdUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GNNs operate on graph data structures, meaning inputs and outputs are defined on graphs\n",
        "- This means a GNN takes in both the graph structure ($V$, $E$) as well as data defined on the graph\n",
        "- Similarly, outputs include a graph data structure + data defined on that graph\n",
        "- In many applications the input / output graph is the same, but features defined on vertices / edges are modified, but this isn't always true\n",
        "- In general:\n",
        "$$ (G_0, X_{V_0}, X_{E_0}) \\to \\text{GNN} \\to (G_1, X_{V_1}, X_{E_1}) $$\n",
        "-Here, $G_0 = (V_0, E_0)$ is the input graph and $G_1 = (V_1, E_1)$ is the output graph\n",
        "- The input graph has vertex and edge-wise feature matrices $X_{V_0}$ and $X_{E_0}$ and analagously for the output graph\n",
        "- This just means that each vertex / edge can have some associated feature vector\n",
        "- Sometimes there are also graph level features as well. These are composite features associated with the whole graph or particular subgraphs"
      ],
      "metadata": {
        "id": "JG02YjGiwsev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Components of a GNN**\n",
        "- GNNs often have analgous components to CNNs\n",
        "- These include **propogation modules** used to propogate information between nodes (these are usually defined similarly to convolutions in CNNs)\n",
        "- **Sampling modules** are used on to reduce data on large graphs (an example is selecting a subset of each nodes neighborhood)\n",
        "- Then, also similar to CNNs, there are **pooling modules**\n",
        "- See [Zhou et al. 2020](https://www.sciencedirect.com/science/article/pii/S2666651021000012) for more information\n",
        "\n",
        "### Propogation Modules\n",
        "\n",
        "- We will focus on propogation operators in GNNs\n",
        "- These operators, often modeled after convolutions, allow for transfer of infomration between graph nodes\n",
        "\n",
        "### **Message Passing**\n",
        "- Message passing allows information to propogate between adjacent nose\n",
        "- GNNs often consist of a series of message passing steps\n",
        "- For example, a message from vertex $j$ to vertex $i$ is given by\n",
        "$$ \\pmb{m}_{j,i} = \\phi^{(k)} \\left ( \\pmb{x}_i^{(k-1)}, \\pmb{x}_j^{(k-1)}, \\pmb{e}_{j,i}^{(k-1)} \\right )$$\n",
        "- Here $\\pmb{x}_i^{(k-1)}$ and $\\pmb{x}_j^{(k-1)}$ are node features input into the message passing layer\n",
        "- $\\pmb{e}_{j,i}^{(k-1)}$ is the edge feature vector associated with the edge connecting node $j$ to $i$\n",
        "- $\\phi^{(k)}$ is a differentiable function, often an multi-layer perceptron (MLP)\n",
        "-Once messages are formed, they are used to update node features:\n",
        "$$ \\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\bigoplus_{j \\in \\mathcal{N}(i)} \\, \\pmb{m}_{j,i} \\right) $$\n",
        "- Here, $\\mathbf{x}_i^{(k)}$ is the new feature vector for node $i$, $\\gamma^{(k)}$ is a differentiable function (again usually an MLP), and $\\bigoplus$ denotes an aggregation function\n",
        "- The notation $\\mathcal{N}(i)$ denotes the neighborhood of vertex $i$, or the set of all vertices connected to vertex $i$\n",
        "- Common aggregation functions are mean, sum, and max\n",
        "\n",
        "### Communication\n",
        "\n",
        "- Each message passing layer means that information can transer 1 \"hop\"\n",
        "- Each layer allows information to transfer over a larger neighborhood\n",
        "- This is a slightly different approach to CNNs where convolutions are sometimes defined on larger neighborhoods\n"
      ],
      "metadata": {
        "id": "RjthT7ZKKsLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Example: Graph Attention Networks**\n",
        "- GATs fit into the message passing framework and use an attention mechanism as in transformers\n",
        "- In an attention layer, first attention coefficients are computed\n",
        "$$ e_{i,j} = a \\left( W \\pmb{x}_i^{(k-1)}, W \\pmb{x}_j^{(k-1)} \\right ) $$\n",
        "- Then the attention coefficients are normalized in a neighborhood of a node $i$ via the softmax function\n",
        "$$ \\alpha_{i,j} = \\text{softmax}_j(e_{i,j})  = \\frac{\\exp ( e_{i,j} )}{\\sum_{k \\in \\mathcal{N}(i)} \\exp ( e_{i,k} ) }$$\n",
        "- Then the normalized attention coefficients are used to update node features\n",
        "$$ \\pmb{x}_i^{(k)} = \\sigma \\left ( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j} W \\pmb{x}_j^{(k-1)} \\right ) $$\n",
        "- Here $\\sigma$ is your activation function\n",
        "- This represents a single head of attention, but multi-head attention is often used\n",
        "$$ \\pmb{x}_i^{(k)} =  {\\big\\|}_{l=1}^L  \\sigma^l \\left ( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}^l W^l \\pmb{x}_j^{(k-1)} \\right ) $$\n",
        "- $\\big\\|$ is the concatenation operator\n",
        "- For more details see [Veličković et al.](https://arxiv.org/abs/1710.10903)"
      ],
      "metadata": {
        "id": "g0AbkcM6seDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Use Case: Emulation**\n",
        "- There are use cases of GNNs in a variety of fields, but we'll focus on their use for physical models\n",
        "- Numerical models can be very slow to run and require a lot of computational resources\n",
        "- For some tasks, like uncertainty quantification, we might want to perform many model runs\n",
        "- This motivates the development of emulators, which are computationally inexpensive substitutes for numerical models\n",
        "- GNNs have been demonstrated for emulating both particle based and mesh based numerical models\n",
        "- For example, [Sanchez-Gonzalez et al. ](https://arxiv.org/pdf/2002.09405) apply GNNs to simulate particle systems based on the material point method"
      ],
      "metadata": {
        "id": "WWIr0klI0m_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/Screenshot%20from%202025-06-04%2010-17-10.png?raw=true\" width=\"800\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "JlaX_sCNvbR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, graph vertices represent particles with features like position, velocity, and material properties\n",
        "- Edges are added dynamically based on distance between points so that nearby particles can interact\n",
        "- Edges have features like the offset or distance between two particles\n",
        "- Uses a message passing network as described above\n",
        "- GNNs have also been applied to mesh based simulations as in [Pfaff et al.](https://arxiv.org/abs/2010.03409), which uses a similar approach\n",
        "- This work highlights that there is a direct correspondence between a finite element mesh and a graph"
      ],
      "metadata": {
        "id": "WCD2KEjiwCoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distinguishing Types of Emulators**\n",
        "- There are many different approaches to emulation with different levels of generality\n",
        "- Some emulators are domain specific, targeting a specific spatial / temporal domain, or a limited set of inputs\n",
        "- Other emulators strive to be more general, requiring either no or limited retraining to apply to new spatial domains and time periods\n",
        "- Both more specific and general purpose emulators have been developed to simulate glaciers\n",
        "- For example, [He et al. (2023)](https://www.sciencedirect.com/science/article/abs/pii/S0021999123005235) construct a highly efficient emulator of Humbholdt Glacier in Greenland using a Deep-Operator Network (as covered by Mauro!)\n",
        "- [Koo and Rahnemoonfar (2024)](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/3BF933067DFFB16949CDADBFE95086AB/S0022143024000935a.pdf/div-class-title-graph-convolutional-network-as-a-fast-statistical-emulator-for-numerical-ice-sheet-modeling-div.pdf) uses a GNN to emulate ISSM for specific glaciers\n",
        "- The Instructed Glacier Model is relatively general purpose emulator for simulating mountain glaciers\n",
        "\n",
        "<br>\n",
        "\n",
        "| Emulator              | Generality | Architecture |\n",
        "| :---------------- | :------------------: | :------------------: |\n",
        "| [He et al. (2023)](https://www.sciencedirect.com/science/article/abs/pii/S0021999123005235)      |   Glacier Specific  | Deep O-net |\n",
        "| [Koo and Rahnemoonfar (2024)](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/3BF933067DFFB16949CDADBFE95086AB/S0022143024000935a.pdf/div-class-title-graph-convolutional-network-as-a-fast-statistical-emulator-for-numerical-ice-sheet-modeling-div.pdf)       |   Glacier Specific   | GNN\n",
        "| [IGM](https://www.cambridge.org/core/journals/journal-of-glaciology/article/deep-learning-speeds-up-ice-flow-modelling-by-several-orders-of-magnitude/748E962A103D2AF45F4CA8823C88C0C0)  | Not Glacier Specific | CNN  |\n",
        "\n",
        "<br>\n",
        "- Emulators like IGM often take advantage of network architectures that exploit local spatial relationships between variables like CNNs and GNNs\n",
        "- Hence, we'll refer to them as local emulators\n",
        "\n",
        "# **The Instructed Glacier Model**\n",
        "\n",
        "- The Instructed Glacier Model ([IGM](https://www.cambridge.org/core/journals/journal-of-glaciology/article/deep-learning-speeds-up-ice-flow-modelling-by-several-orders-of-magnitude/748E962A103D2AF45F4CA8823C88C0C0)) drastically speeds up simulations of mountain glaciers\n",
        "- Estimates ice flow velocity from geometry, basal traction coefficient, and the temperature dependent rate factor\n",
        "- Approximates ice flow velocity using a fully convolutional network\n",
        "- All inputs / outputs are defined on grid\n",
        "- IGM resolves vertical velocity profile with 10 vertical spatial layers\n"
      ],
      "metadata": {
        "id": "T0XQsdoRzDGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://github.com/glamacles/notebooks/blob/main/images/local_emulators/igm2.png?raw=1\" width=\"800\"></td>\n",
        "    <td>\n",
        "\n",
        "| Variable              | Symbol |\n",
        "| :---------------- | :------: |\n",
        "| Ice Thickness       |   $h$   |\n",
        "| Surface Elevation         |   $s$   |\n",
        "| Rate Factor   |  $A$   |\n",
        "| Basal Traction Coefficient |  $ c$  |\n",
        "| Grid Resolution |  $H$  |\n",
        "| Velocity X-component | $u$ |\n",
        "| Velocity Y-component | $v$ |\n",
        "\n",
        "  \n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "jVPCGDDH-N1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Note that IGM only emulates the velocity solve in the ice dynamics model\n",
        "- Solutions to the mass continuity equation and heat equation are computed efficiently on the GPU using finite differences"
      ],
      "metadata": {
        "id": "azRHeWvS9Kkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Physics Informed Loss Function**\n",
        "\n",
        "- Solving for ice velocity can be posed as minimization problem\n",
        "- The varitional principle for momentum balance is given by\n",
        "  $$ J(\\pmb{u}) = \\int_{\\Omega} \\left [ \\frac{2n}{n+1} \\eta (\\dot{\\epsilon}^2) \\dot{\\epsilon}^2 + \\rho g \\pmb{u} \\cdot \\nabla{s}   \\right ] \\; d \\Omega + \\int_{\\Gamma_b} \\frac{c}{m+1} (\\pmb{u} \\cdot \\pmb{u})^{\\frac{m+1}{2}} \\; d \\Gamma $$\n",
        "- Taking the first variation $\\delta J$ yields the weak form of the first order ice flow equations\n",
        "- This serves as a natural way of defining a loss function for training the CNN\n",
        "- In IGM the strain tensor $\\dot{\\epsilon}$ is simplified using the first order Blatter-Pattyn approxmiation\n",
        "\n",
        "## **Training**\n",
        "\n",
        "- IGM has been geared toward simulating mountain glaciers and ice fields\n",
        "- Hence training data involves simulations on mountainous terrain in the European Alps generated by CsFlow or PISM\n",
        "- IGM does not need training data per se, but it does need a wide variety of input scenarios to train a generallly applicable emulator\n",
        "- IGM has a collection of pretrained emulators\n",
        "- It also allows for dynamic retraining to improve accuracy if needed"
      ],
      "metadata": {
        "id": "pg94DFNA9vxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IGM-Like GNN Emulator**\n",
        "\n",
        "- IGM is designed for an ice flow model that uses a uniform grid\n",
        "- Many models, like ISSM, are defined on an unstructured mesh\n",
        "- This motivates using an alternative approach that can handle models defined on non-uniform meshes\n",
        "- Intuitively, finite element meshes can be easily interpreted as graphs\n",
        "- Mesh vertices correspond to graph vertices\n",
        "- Mesh edges correspond to graph edges\n",
        "- In [Koo and Rahnemoonfar (2024)](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/3BF933067DFFB16949CDADBFE95086AB/S0022143024000935a.pdf/div-class-title-graph-convolutional-network-as-a-fast-statistical-emulator-for-numerical-ice-sheet-modeling-div.pdf), the finite element mesh used by ISSM is translated directly to a graph\n",
        "- Variables, defined on mesh nodes become vertex features\n"
      ],
      "metadata": {
        "id": "0UoQUaS8FK6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/Screenshot%20from%202025-06-04%2012-10-08.png?raw=true\" width=\"800\">"
      ],
      "metadata": {
        "id": "RSI5AlGTSS5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A GNN Emulator for SpecEIS**\n",
        "- [SpecEIS](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221861) is a another ice flow model that uses particular finite elements for stability and mass conservation\n",
        "- To emulate SpecEIS, it turns out to be more convenient to define a graph based on the dual mesh\n",
        "- Graph vertices are defined at cell centers\n",
        "- Graph edges are defined between cell when two edges share a common edge on the standard mesh\n",
        "\n",
        "<img src=\"https://github.com/glamacles/notebooks/blob/main/images/Screenshot%20from%202025-06-04%2013-16-20.png?raw=true\" width=\"400\">"
      ],
      "metadata": {
        "id": "eEKpzyOcFjZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vertex features (defined on cell centers) include the basal traction coefficient $\\beta$ and ice thickness $H$\n",
        "- Edge features include jumps in the spatial coordinate $\\pmb{x}$, bed $B$, and thickness $H$.\n",
        "- These are analagous to directional derivatives\n",
        "- The output of the model is velocity, which is defined on edges for compatibility with SpecEIS"
      ],
      "metadata": {
        "id": "WsSrUrtiYjL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**\n",
        "\n",
        "- The GNN SpecEIS emulator is trained on simulations of mountain glaciers throughout Montana\n",
        "- Similar to IGM\n",
        "- Perhaps due to using unstructured grids, the emulator requires additional measures for stability in time-dependent simulations\n",
        "- This includes adding noise to the training data and performing \"rollouts\"\n",
        "- This is where multiple emulator time steps are used to evaluate the loss function\n",
        "- With these tricks, the results are mostly pretty good"
      ],
      "metadata": {
        "id": "1WbP8u9mo7oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"1000\" height=\"600\" src=\"https://www.youtube.com/embed/-q8n2ivSxAs\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "oVF0KY0q8I9J",
        "outputId": "fe72470f-ed14-4ea9-97b0-edc8590a926f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"1000\" height=\"600\" src=\"https://www.youtube.com/embed/-q8n2ivSxAs\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Some Big Challenges**\n",
        "- The GNN approach on unstructured grids seem to have some innate challenges not encountered by IGM\n",
        "- For instance, tests on ideal geometry do not exhibit the expected symmetry\n",
        "- More effort is required during training to ensure stability in time-dependent simulations\n",
        "- \"Wiggles\"\n"
      ],
      "metadata": {
        "id": "fKzuOYgW9JCV"
      }
    }
  ]
}